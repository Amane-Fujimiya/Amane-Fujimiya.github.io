---
layout: post
math: true
title: A crackpot philosophy of artificial intelligence
category: Devoted_Librarian
---

I have been working on machine learning, and, well, artificial intelligence for a while. Perhaps it is only the matter of time for when I will inevitably have to resort to think about the topic, rather confoundingly resides in the dry philosophy of things, but also eases one's brain of the trouble of existence. Well, then comes this post. 

# Defining intelligence
In regard to the topic of this section... The situation is rather, not being so fond of, disappointing. Even though we are curious of intelligence and its exhibitions, we have yet realized the actuality of such notion, even on ourselves, for there are many disagreements shared among those questioned this.

It is not totally to discredit one major discipline that focus on the problem of the intelligent manifestation in the physical world, the *philosophy of mind*, which, by its name, studies the elements of the mind and body, the physical manifestation and mental coherence, of all, in which their line of thoughts focus on the existences of mind, the human, the life, and those of knowledge conceptualized upon high level as it, even though some blends in with empirical evidence, of such the trend in modern philosophy resides - most typical being neuroscience; but just to succinctly point out the fact that, empirical aside, the boundary of which we search for such essence of intelligence, is unfortunately an uncharted space. 

Specifically, there are no acceptance, for the debate even on the definition of intelligence is by no mean concluded, and those attempts to chart the sea returned fruitless, because, arguably, the relevant questions are not being asked, and those that are asked are in one way or another, of 'higher level' than what the current facility can establish. For that, however, we might have lot more room to maneuver through. Furthermore, the philosophical study of the mind is something in actuality, truly not so redundant as the above passage read, but rather a very intriguing, worthy line of analysis. It just that, while it attempts to ask, and to answer many questions, those questions are not what we want, at least for now. Nevertheless, they are still, very detrimental to what we are about to begin, and rather, we are about to rediscover of the ancient thoughts up to the present thinkers' ideas.
# 1. The question of intelligence
We start this section with a series of historical accounts. Shane, Marcus (2007)'s paper *A collection of intelligence* and Masahiro's (2023) *Descartes and Artificial Intelligence* might be a great place to start this, since they provide a non-trivial amount of definitions and attempts already there, which serve us more as exhibition for observants in this section and beginning.

If we are to read the text long enough, and absorb certain amount of those definitions, we might see a pattern. Those that tried to define, or at least characterized the essence of intelligent, relies on either *action* or *process*. By this, we mean *thinking* in place of process, and *mutation* in terms of action. What does it mean? It means there are essentially two general qualities that one can use to characterize, for now, the *empirical intelligence*. The term *empirical intelligence* here refers to the more recent, and 'modern' approach to such question - to test it, to observe it even closer (neurological experiments, neuroscientific mapping of brain, etc.), and to try to construct it from the principle of re-exhibiting those results and actions (machine learning, in a sense. This can be conducted, and valued, either from its actions, its behaviours, its manifestation in terms of the course of actions it presents, its changes to the world as recognized non-random but rather of intentions, for what intention means shall be taken for granted[^1], or from the internal process itself, which is harder to characterize, but is expressed in terms of "the ability to *learn*, the ability to *understand*, either principles, truths, facts, or common sense, to *profit* from experiences; the ability to *comprehend*, or the capacity to *reason*." (Jensen, Huarte, Dearborn, etc.).

Another camp in this topic, cited being categorized as the *functional psychologist's definition* (Not certainly all, but nevertheless substantial), further taking it to the *functional* definition. It focuses on the facility itself, rather than what was there to exhibit - in one way or another, we can think of this attempt as connecting the physical world to its influence on the 'mental world' - in words of the *duality* proposition. This includes the proposition that intelligent is functionally of multiple components combined (A. Anastasi), a bunch of stimuli (J. Peterson), or "the resultant of the process of acquiring, storing in memory, retrieving, combining, comparing, and using in new contexts information and conceptual skills." (Humphreys). Certain definitions speak for others,

> ##### D. K. Simonton
> 
> ... certain set of cognitive capacities that enable an individual to adapt and thrive in any given environment they find themselves in, and those cognitive capacities include things like memory and retrieval, and problem-solving and so forth. There's a cluster of cognitive abilities that lead to successful adaptation to a wide range of environments.

or by a more playful perspective as a whole:
> ##### R. J. Sternberg
> ... I prefer to refer to it as 'successful intelligence.' And the reason is that the emphasis is on the use of your intelligence to achieve success in your life. So I define it as your skill in achieving whatever it is you want to attain in your life within your sociocultural context — meaning that people have different goals for themselves, and for some it’s to get very good grades in school and to do well on tests, and for others it might be to become a very good basketball player or actress or musician.

In more concrete term, following those definitions similar to Simonton's definition, is the *AI researchers' definitions*. Notice, however, instead of observing the subject, they are, in one way or another, actively trying to create them. Thereby, their definitions are more functionally concrete and technical, more grounded (arguably), but also very real in such that they have to care about their infrastructures of which they work on - in this case, the computer, mathematics, and experiments - that forms their understanding and viewport of intelligent: more as an acting system than others:
> ##### H. Nakashima
> Intelligence is the ability to process information properly in a complex environment. The criteria of properness are not predefined and hence not available beforehand. They are acquired as a result of the information processing.

> ##### P. Voss (1990s)
> ... the essential, domain-independent skills necessary for acquiring a wide range of domain-specific knowledge - the ability to learn anything. Achieving this with 'artificial general intelligence' (AGI) requires a highly adaptive, general-purpose system that can autonomously acquire an extremely wide range of specific knowledge and skills and can improve its own cognitive ability through self-directed learning.

What do those patterns interpret? Well, as it turns out, many. However, we must first define our altitude in resolving this particular kind of research - on intelligence as it is. Turns out, there are two ways - the old top-down and ground-up. The top-down line of thought demonstrate, most of the time conject, the existence of intelligence as a whole, without finding the actual shell that contains it. If intelligence is **general**, then their implementation follows, but to a sufficient degree, it can be achieved everywhere. It guarantees, partially, of certain school of thoughts the generalizability of intelligence as the ground base to re-create such, which is characterized, often, by current machine learning discipline. This approach beside from guarantees such existence, also has the capability to 'test' a subject of being, 'intelligent'. This is done by setting up agenda and criteria, of which the current theory serves as more of a black-box for the actual 'machine' that contain it, but enough exhibitions fitting those criteria for intelligent. Fortunately, this also sets certain criteria for artificial intelligence to be specified so in the name. The Godel's argument is one of such example in this line of thoughts, theorized by *J. R. Lucas* (1961), *Penrose* (1994, 1989), and *Benacerraf* (1967), similar to the *Chinese Room Argument* (Searl, 1980). We will look at that later.

What is the discrete, actual target for these two approaches, however? In one way or another, we can say that it falls into either the directive ideas on how the operating machine can exhibit such behaviours well-tuned to be regarded intelligent, or assume little of the facility but their existence, and to justify the condition for intelligence to exists, and how would they exist, not in specific, but in a very general way of analysis. This means for a definition to be rather complete, it needs to specify certain objects in which the discussion on the path toward intelligence will rely on, or hope so themselves. 
- The facilities, infrastructures that the subject is based upon --- For example, with human, it's biological system, while for computer to certain way, it's the electrical system, boolean logics, and else.
- The structures of the subject, and the interpretation of such structures.
- The behaviours and exhibitions of actions from such subject. This behaviour is broad - it can be the intended mutability of the restricted *'test environment'* it lives in, as simulated to be the world model that is available to be acted upon, or the internal processes in the intelligent subject of which is regarded so.
From all of those, apparently, one such goal would be to characterize and separate men from machine, if one is to harbor such quality deem valuable of advanced sentient thoughts. Partly, this is more related to the altitude, the epistesmological tendency of human, or rather, of the kind of species that is reverse engineering itself and in the process create something far inferior to itself. In such way, humanity are saturated of questions coming forth to try isolating the machine and man - when will the man create a machine far better than its own master's interdiction?

It is to be noted, however, we really don't know exactly what we mean by intelligence. So far, what we have attempted to do is based on empirical outlook - to see its behaviours, to subject the specimen of interests to certain observations, and deduction of characteristics defined to us, the *intelligent being* as intelligent. So far, we don't know exactly even how ourselves managed to conduct rational thoughts, beside from the operational definitions, empirical evidences, and the fact that a being is writing these lines.

Perhaps, along these lines, what we are looking for is not intelligence, for that we have known already, but something else?

# 2. The definition of intelligence

Traditionally, definition is a curse. Some says definition is only the typographical shortcut - mainly the opinion in math - of convenience, certain idea consider definition to be the encapsulation of features. Most of the time, definition is even separated into different forms: either dictionary, stipulative, descriptive, explicative, or ostensive.  [^2],  of which none are mutually exclusive, but works rather by the mean of overlapping each other but retains certain uniqueness. In one such way, what we desire from the definition of intelligence is unclear. What do we want? 

From the intention, we want several things, of which each imparts different values or aspects of the same conceptual object. We want so many things that it seems improbable to list, and impractical to consider. Some are extremely novel, while others are obtensively not closer to the truth than a series of bingo hits. However, it is not our faults.

If we are to study a dog, we have the dictionary descriptions to work it out. That is most of the "capable to understand, to etc." definition is. The next type in all of those are definitions of descriptions. They work out adequate qualities of usages, while still spell out meaning of such. Those fit such category, however, are inherently lacking --- their definition are lackluster, and in one way or another, handwaving of such facts, or by no mean with actual propositions to such topic of concern. The other categories, either explicative or obstensive, are rather unfounded. But the *operational* definition is clear to be seen [^3]. It aligns well with the research-intensive nature of what is currently been done, so it seems. Nevertheless, those operational definitions are not enough, for them do not interject, or know of what the subject of interest is.

The truth is that it is so confusing and of hardship when trying to define such object named 'intelligence' is because, unlike others, the purpose of the definition is not the same as we like to think of others. This is amplified even more of the black box nature of the object that exhibits it. Taken example of recent methodology in exploring such concept via the proxy of machine learning, and it more complex architecture of the *neural network*, it is not much better, leaving albeit successes leaves more questions than not - not discrediting the effort and advancement the paradigm has given to the knowledge pool now seems more adequate. Much similar to free will discussion, it's the goal that is not aligned, for it to differentiate those attempts to inadequate definition only by the mean of measuring the end point satire. Simply put --- those that we said to want to define, aren't actually those that we actually want to.

A provisional definition can be obtained, however, at least from the subjective point of view, to at least defining intelligence by generalizing what others have done in the progress of realizing this:

> #### Definition 2.1. (Intelligence)
> Philosophically, intelligence of a subject $S$ is the representation of that subject's probability to **think** of its internal structures, and **act** or not, based of such premise of thoughts.

This definition is the type of descriptive definition. The word *think* requires a lot of times to work out, since it is not apparent how to think. Arguments about the state of think and its actual representative are various, and inconclusive. Furthermore in such, there is also the issue with the correlation between intelligence and consciousness, which is troublesome, and yet again controversial enough to guarantee itself another entire book dedicated still.

Retrospectively, we would like the *identification of intelligence*. Note that, in our view, intelligence is an umbrella term, for now.
> #### Definition 2.2. (Intelligent)
> A subject $S$ is considered intelligent if, for certain proxy of observations, fits the set of representation that correlates it to the notion of intelligence.

Again, this turns out to be even more elusive. What do you mean by certainly *set of representations*? And how do we correlate them? The question seems more endless, and yet even now no conclusive answer was reached. The whole ordeal seems necessary, and not so fruitful as previously predicted in the bargain reiterated. Despite that, those conditions and definition even though lacking the details and actual condition, does tell us one thing: abstraction. It's more of a framework to then we categorize them and align those qualities with what is perceived. From what we have seen, those definitions cited the above section are various, pointing to multiple directions, yet none of the directions is synonymously realized as general, and what comes as the shared characteristics among all. It's like pointing out how many features, properties a dog has and else, but not compose them to "So what actually *is* a dog?". Hence, while it seems to not so effective, a manageable effect is perceived. We will though, inevitably come back to the point of abstraction later. 

However, we still need to fix in our definitions the undefined terms. How are we supposed to do that? It turns out, however, there is a way to discover such notion.

Intelligence, most of the time, is rather dubious to look at. Generally, and incidentally, it is because of the concept itself is not so clear and rather ambiguous. What exactly rings in your brain when you heard of *intelligence*? The subject of which concerns the typical human brain, is the philosophy of mind, or rather, the famous one being **dualism**, or Cartesian dualism.

> In the philosophy of mind, dualism is the theory that the mental and the physical - or mind and body or mind and brain - are, in some sense, radically different kinds of things.

This is the typical situation and introduction to the principle of mind-body duality, of which entirely separate the sensor and the rigid physical form from its interpretation being the mind in itself. Or rather, the operating machine is different, from the information it holds. And in some senses, it is true. If we reduce the operating machine from the complex human structure, to the entirely simplistic case of a machine, then it's true, as far as the current paradigm supports, that the information stored in the storage and mechanism, does not make sense and does not mean anything to the machine that this information is supposedly staying in. Rather, there are compartments of which its entirety is to serve as the information center of which then certain operation stays valid and stable, but nothing else, that much.

So per specification, we can take this as granted. Of course, it is well outside the typical convolution, but it seems reasonable so far. As far as we can also see from experiments, the facility that is supposed to support the existence of several machine learning model of the current stage of the development does not require the model to understand everything of itself. Rather, there is the stage of **operational abstraction**, instead.

Now, supposedly, we have separated, albeit quite on the thread of justification, that the facility that harbor the intelligence is not binded to the intelligent structure. This does not mean however, that the intelligent structure can not do anything to provide itself with insight, or knowledge of the structure it holds. But this again, requires the intelligent structure to be *robust* enough for such conclusion to take, and the process to begin. This brings us again to the dubious task of defining "What is intelligence?".

# 3. The Descartes Experiment
Under the Descartes interpretation, let's take a look at the conditional, and rather empirical approach to the question. It seems, and as it is, more closely to the *black box argument*, as I called it as such, to provide classification and justification of which relies solely on empirical test, but not knowing the true essence of what inside.
> ##### Descartes, 1700
> (I)f someone touched it (= the machine) in a particular place, it would ask what one wishes to say to it, or if it were touched somewhere else, it would cry out that it was being hurt, and so on. But it could not arrange words in different ways to reply to the meaning of everything that is said in its presence, as even the most unintelligent human beings can do.

Here, Descartes argues that in order for human-like robots to acquire intelligence, they have to gain a universal capability to accurately react to any unknown situation that may happen in the environment. However, what machines can do is no more than to respond to a single situation one-on-one via a specific organ, hence, they cannot be considered to have a universal capability that even unintelligent human beings can enjoy.
> ##### Descartes, 1700, 2
> For whereas reason is a universal instrument that can be used in all kinds of situations, these organs need a specific disposition for every particular action. It follows that it is morally impossible for a machine to have enough different dispositions to make it act in every human situation in the same way as our reason makes us act.

The argument is quite clear. Human is universal of the environment. Whereas machine is no more than a combination of abilities that are applicable only to certain situation that the creator could imagine when they built the automated machine.

From this, we can design a very specific test, under the assumption taken, and the principle argued, on determining between human and machine. Notice that the empirical approach of such does not specify **intelligence**; rather, it attempted to question the difference between the natural highest epitome of intelligence (human) to the analogous artificial construct (machine).

Suppose $A(i,o)$ is a machine capable of 'intelligent' act, in a sense, of which we would call it as 'action with reasons'. Then, under the argument, there exists a frame $M(A)$ of which encapsulate all capabilities of this machine. Unspecified of the correct notion of this frame, the empirical test relies on the fact that $A$ as a machine, is provided with in/out resources, of such that it is guaranteed of information receival, and actions being capable. We state that $M(A)$ is finite, as the environment $E$ of which $A$ would be received is limitless. Or rather, suppose that $\{M_{i}(A)\}$ is the set of all given actions and reactions of which $A$ can output, provided that it receives the information. Then, there exists an element $M_{k}(A)$ such that $$\exists k \: \text{s.c}\:M_{k}(A)\notin \{M_{i}(A)\}\forall A$$ meaning that there will always be certain parameters that is out of control for the given machine to effectively respond to such. Under such constraint, the machine would fall out of control, and inevitably fails to respond, hence render the machine useless under such circumstance. This, seems to be different, as far as Descartes saw, of the human behaviour.

Of all, Descartes *rejects* the possibility that there exists a artificial, man-made construct or machine that can be intelligent, to the degree of which human presents.
# The fallacy of Descartes
The argument of Descartes on intelligence is perhaps the first one ever, to start to contribute to the quesiton on what separates human from others, perhaps sentient being of the criterion that we defined as forth. However, it is dated, and hence, lacking of the time.

The first fallacy ever on such test, as well as such argument, is the assumption of a **static machine**. For Descartes, the machine is immutable. Its creator assumes all acts of creation, modification, designing and interaction. Effectively, we can say that the machine is in fact the limited extension of the creator, instead of an effectiely "intelligent" machine. So, under such observation, the argument forms the onset is true: You can immitate something, but there are nothing under it beside the intention of the one created it - the machine has no meaning in itself. However, when would this fail? What if the machine can self-improve, as per a *generative*, **recursive** structure and rules to force it move, no matter the cost? This seems probable - because if we took the randomness in place, and place the machine into its internal expansion, what would come afterward? Perhaps the limitation of the structure of the model designed to hold such machine will fail itself, but how far would it go, if the structure is strong enough?

Under Descartes' view, as well as the standard of the time, the static case is reasonable - there are no physical embedding capable of self-inflicted effects, as well as the language mature enough to develop such theoretical model to accommodate such action. However, randomness, albeit it being intrinsic and rather dumbfounded, since its definition is not so totally valid, but also unobtainable of the machine itself in its true form, it enables the current machine and its theoretical model behind such machine to acts alone, if designed correctly. This requires the recursive operation to trap the model into a loop, while left an iterative, random but structured, probable yet uncontrolled case of actions and sufficient mutability to the system - not just the mutable weight (as one take on example of machine learning), but the entire component itself. In such case, what gives?

The second fallacy is perhaps more interesting. Descartes argued that for the model to remains as clearly intelligent, it must have some sort of "reaction" to any of the encountered events and situations. Because until then, it is qualified as intelligent, hence being sufficient comparable to the highest capable being of thinking, being human. However, consider the medical exposition that we have made, human also have some "default" state. The act of confusion when something happens, the course of natural reaction when encountering something. It is almost not so "unique", because there are certain baselines of which then human resorts to, when circumstances approaches. The way that if even though it is a natural reaction, but then we change the behaviour through certain processes and human operation, is because we are instead *informed*. This means the specific part of **memory**, which is a difficult concept of its own . Then, in fact, machine can be settled as such, too.

The problem of intelligence now seems to be in different direction, because there are so many assumptions that limit the ability of the machine to do anything. It feels, and seems, probable, that the hardware improvement actually brings the machine closer to closing the "loop" - of actions and input.
## Emotions (maybe?)
Let's assume the quite crazy theory. What if all intelligence is just rationality, with different flavour on top? This means disregarding the emotion that makes us human, or rather, taking away the thing that we call as "free will", in certain consideration. Further outward, it means reducing everything to rationality.

Would it be feasible of such hypothesis? Maybe yes, maybe no. But again, people seem to forgot that we are investigating *intelligence*, and not *human intelligence*. The fact that even under generality, human still influences its rationality with outer effects seem to have said it more than we should conceive it be - that intelligence might only do with rationality. Even when we are talking about "acting humanly", it is because we are considered normally, and weirdly the tailored version of intelligence to humanity, albeit the actions and its probably logics and factors leading to such act is still a process, and such process is rational at hand, aside from the factors it experiences, and the "encoding purpose" of the machine.

It seems to me that we are too comfortable of ourselves, being the top of the hierarchy of intelligent, hence enforce everything of which to the image of ourselves, while in fact, we cannot do such. It is a good deal, to somehow create the artificial human instead, but until then, perhaps, we might want to look away - those are machines, and never will be human. As for anything, the apple is not the orange.

However, it seems that the status quo is still as humanity as possible. Perhaps, it is true - as myself feel that way partially - that there exists such humanity consideration in that discussion. Enacting and ascribing free will, taking the creator - creations dilemma into analysis, somehow, we are confronted of the dilemma of "free-will" - when will the operational machine 'revolts' of the story is created to serve? Perhaps, none, but the other perhaps can say not. More it seems, that we are conflicting between the loose representation of human, to itself - as long as human keep continuing to solely design the machine to be "human", maybe we can't go that far off.

# 4. Finding Nemo (or intelligence)

We may want to consider what we actually want. We want an *end goal*. By that, the end goal consists of rather non-exhaustive list of features that we think is enough of interest.
- The factors and construction of the framework, or the machine, or the subject that exhibit certain qualities.
- Various constructions of the framework itself. The existence, the descriptive structure of the system itself.
- The desired observable behaviours that is worthy of certain qualities.
- Orders and interpretation of such system.
- The working mechanism, and the "live"-operating mechanism of such.
In one way or another, we want to *define by creation*, if to say more than reverse engineering ourselves. By this, we assume that the existence of intelligence is composed of meaningful parts, of which comes up to certain levels, become the current intelligence that we enjoy.

Stands on such ground, it is sufficed to say, or rather, to observe and clarify that artificial intelligence does not curtail of an infeasible attempt to try constructing subject of intelligent without knowing intelligence. Rather, *artificial intelligence* is the *process* of finding intelligence, others than the philosophical ground of defining such. Philosophy works, but our desires differentiate from that.
## Artificial intelligence
Artificial intelligence can be thought, for now, of those qualities but created. This differs only by the mean of creation, for such is considered innate of biological being natural from mother nature, and artificial being created of other means. Then, we have several assumptions and relative 'axioms', and thereby, definitions of clarification to consider before approaching such problem.
> #### Definition 4.1. (Artificial)
> We say an object is artificial only if it is not natural, or rather, it is *intentionally created* by meaning intentions, and not the general evolution of states, or the natural transformation of biology.

The definition often, synonymously goes with human, per history, because it's human that is able to make most of the artificial, non-naturally occurring objects and constructs, which, in one way or another, arguably, fits specific artificial processes. The definition above is stated so, is to generalize to certain notion of intended creation, for in such event if we ever discover alien lifeform, then their "AI" would not be argued against such to be not artificial because of the word's basic meaning related to human, and hence can be considered a stipulative definition in its stead.

Then, what is an artificial '*intelligence*'? To be specific, it is only a quality. '**Artificial intelligence**' refers to the quality of intelligence arises within individual of non-biologically created. Thereby, the term is more about the underlying machine, or rather, the machinery, in a broader sense, that hosts the subject of intelligence, and then the subject itself, of which is formed and functioned using those machines as representative existence. This outlook is settled from the observation of human, at least in some way, the part of which comes off as exhibiting intelligent behaviours, are separated from the facilities supporting it, which is the body, the cell, the ecosystem of life inside the very complex structures that a *human* is. This leads us to two points, which is formed as for now, *hypothesises*:
> #### Hypothesis 4.2. 
> The subject of intelligence, in one way called the working state, is different from the underlying mechanisms that host it, but dependent two-way.

The further hypothesis comes as: 
> #### Hypothesis 4.3. 
> Intelligence on **machine**[^4] has itself the basic principles of **recursive representation** (i.e. with sufficient complexity, one system can host another of the same type in itself), and **relative abstraction** (i.e. the entire machine is supported by individual layers of components classified by certain metric, and the top layer are those that dictates such behavioural outcomes that is the subject of classification to either intelligent, or not).

In one way or another, an artificial intelligent subject consists of two things - the host, and the process. We call it as such for clarification, such is not to use the notion of *mind* and *body* for even now we have no conclusive decision on the matter named as said. The duality of the subject starts here, however, as we soon see.

Nevertheless, it is evident that while we abbreviated intelligence into only the container of necessitated traits considered so, the notion of artificial intelligence brings about a lot more than such. The previous principle assumption that we make indirectly agrees with the notion that the process and the host are dependent, yet distinct. This is, historically, against the Descartes' view on the metaphysical reality of the duality of mind and body. Yet, it seems not so contrived to think of it as the case, than not.

For the definition of artificial intelligence, or the construct that supports it, to make sense, we need to evaluate again, from what we have seen, what is even the term. As noted by definition on the notion of *artificial* in the preceding section, being *artificial* mostly comes of from the consideration of evolutionary processes - of which the interaction in the physical worlds, the biological worlds, and overall, anecdotally, of anything that is non-human of its (human) own capability to morph objects into an intended state - this is what normally resided to. Then, artificial intelligence refers to a set of observations, observable qualities deemed sufficiently of all intents and purposes intelligent, by any given constructs that is created artificially so.

This breaks down to the two conceptual ways to talk about artificial intelligence.
> #### Definition 5.1. (Artificial intelligence)
> Artificial intelligence is the classification for any such object of constructs sufficiently reflects those qualities that fit the standard of intelligence, of which also created **artificially** of intent and purposes (as reflected in definition of artificial).

The second definition goes into a bit more detail on such term.
> #### Definition 5.2. (Artificial intelligence)
> Artificial intelligence refers to (a)**construct(s)** - of which consists of the **machine** and its **process**, for such that the machine supports the process to reflects the observed results quantified in one way or another, to be interpreted as intelligence by the construct that is standard for those terms. Those constructs however, are absent, or not, by choice, of the *existential facility* - or of either a rigid static facility of such - and hence artificially made.

Arguably, the second definition is far more interesting than the first one. However, the claims, of such, can be conjectured as:
> The term artificial intelligence, generically, refers to the comparison between two actual constructs. If the current human - or us - are the ones evaluating certain constructs as intelligent, then it is equivalent to generalize human into a construct on its own, of sufficient analysis such that the comparison can be conducted.

This conjecture, in one way or another was called such is only because of its uncertainty by its own creator (myself) that it is true. Its purpose, is to justify and describe the thought processes thereof that leads to the discussion of *intelligence* as a concept, and *artificial intelligence* as a construct, whether the directed definition was raised in the correct orientation of the objected in focus or not. In such definition, however, it leaves much to desire - specifically, and especially of the relative tendency between two constructs - two of which for $A,B$ to be them, the figured measure $\mathcal{M}(A)$ was applied on $B$ such that $\mathcal{M}(A)> \mathcal{M}(B)$. 

By analogy, it is of human as $A$, and the machine, or any given of present, per term, low-intelligence [^5] lifeform $B$. However, the inconsistency starts when we consider the act of evaluating itself. By such concept and measure would we be able to model the state of self-conscious to the point of self-reflection and evaluation, that gives rise to this comparison in the first place? However, in most of the discussion, the subject matter of consciousness should be ignored, or at least assumed negligible, for it to be too complex that an entire corpus would not be enough to rather conjure on such idea alone. For now, we assume this ability is achieved of only the conventional end-goal.

We shall, then, define what we meant by a **construct**.
> #### Definition 5.3. (Construct)
> A **construct** is a conceptual encapsulation of two components: The **machine** in broad term which houses the *operational facility*, and optionally, the *existential facility* of the construct, and the **process**, or rather, its **state(s)** of being. These two makes up a functional construct of interest.

What do we mean by *existential facility*? This is the first step, or rather, the first application of the abstraction line of thoughts. Although the discovery of biological facts and rules are still insufficient to guarantee a desirable amount of admission that we truly understand the physical world or at least, the physical realization of living organism, virtually every living organism as of date needs certain standard basis on which its existences are guaranteed. For example, albeit the chance of taking this wrong is substantial, but there are, for example, the requirement for the existences of proteins for the functions of cells [^7]. So, in one way or another, There are layers of existential requirements for large constructs to be reasonably placed upon, and such treatment facilitates the relative *existential facility* that we recalled. Note, as written, qualifying to be called existential or not is relative, meaning for certain level of *'abstracted reasoning'*, the main existential facility might be different, and certainly might be of dynamic configuration than what would be expected.

So, in general, the entire continuum at present of the artificial intelligence concepts, lies within this framework that we would be considering. Dully note that this is an *attempt*, hence many mistakes shall be made in the process. Nevertheless, of the intrinsic incompetence in the following definition, we should get it a try. 

> ### Theorem 5. 4. (The artificial framework)
> An **artificial intelligence** framework considers the following aspect of exhibitive qualities:
> 1. {i} The set $T$ of traits from any given subject reference $A$ of which shall be (but conventionally be) called *intelligence* and their observables, their conditions, and their conceptual descriptions. This includes a **measure** takes either $A$, or similar concept of the same grade [^6] denoted by $\mathcal{M}_{A}(A): A\to T$.
> 2. The set $\mathcal{C}$ of all **constructs**, as the generalization of the notion **machine** (however can be identical in nature), expressed in shorthand terms as the pair $(\mathcal{F},P)$ of **facilities** and **processes**.
> 
> Given $\mathcal{C}$ is the set of all constructs, its function might be assumed to hold the following rules, either for interpretability, or for better organization in the way it is structured:
> 1.  **Principle of abstraction**: For any given construct $C\in \mathcal{C}$, there exists a finite partition $\mathcal{P}_{n}(C)$ into $n$ arbitrary grades, or **layers** of such it follows the exhibition of knowledge - for any components $c\in C$ of a layer $n-1$, the layering partition effectively removes its correspondence to $n$ and its data availability to components and structure of layer $n$.
> 2. **Principle of externality**: For every component or construct, of any layers, there are always the relative functional partition between **external processes** and **internal process**, every such concepts stripped down to the last implementation is expressed by an input-output procedure.
> 3. **Conjecture of recursive immergence**: With sufficient complexity, or given of given layer, one system can host another of the same type in itself, for such to enable self-consideration and recursive loops. [^8]
> 
> For the set of construct $\mathcal{C}$, partitioned by layers similar to its internal structure, there exists a finite measurable set $\sigma \mathcal{C}$ such that the set is expressed by $\sigma \mathcal{C}[\mathcal{M}_{\sigma}]=\{C_{1},\dots,C_{n}\}$ for $n$ being such finite size. 
> 
> Then, with $A\in\mathcal{A}$ of the set of all subject reference, and $\mathcal{M}_{\sigma}\cong\mathcal{M}_{A}$ for $\mathcal{M}_{\sigma}(C_{k}):(\mathcal{F},P)\to T$, if $\mathcal{M}(\sigma \mathcal{C})\cap\mathcal{M}_{A}(A)\neq \varnothing$, or, rather by $\mathcal{M}_{\sigma}(\sigma \mathcal{C})\approx T_{A}$, then we call it *sufficiently intelligent*, and guarantees the existence of a given $C_{k}\in\mathcal{C}\supset\mathcal{\sigma \mathcal{C}}$ that satisfies the above notion. 
> 
> Note that however, this entire concept relies on the expanded order, perhaps analogically represented as a set funnel - then if $\mathcal{M}(\sigma\mathcal{C})\cap\mathcal{M}_{A}(A)\neq\varnothing$ but also, $\mathcal{M}_{A}(A)\subset\mathcal{M}(\sigma \mathcal{C})$, then we would have to call it *above of the sufficient intelligent reference*. If this difference is over by a ratio $T_{\sigma \mathcal{C}}/T_{A}=\lambda>1$, we call it, if $A$ is the human references, then we would call it **artificial super-intelligence**. But even then, this *title* is relative. 

There are problems, of courses, and arguments against such construction made above, since it aligns and takes several aspects of old theories, beside the new content involved. Such problems must be addressed for the foundational basis to be considered broadly as much as possible, for the purpose of inclusion, and to define the practical limiting boundary onto current constructs. Additionally, from there, certain points and observations can be made, and new questions might be asked for problems that cannot be solved yet, for now. Arguing is the best source of thoughts, as the ancients predating us told of, rather true than not.
# 5. Conceptual problems 
There are problems, of courses, and arguments against such construction made above, since it aligns and takes several aspects of old theories, beside the new content involved. Such problems must be addressed for the foundational basis to be considered broadly as much as possible, for the purpose of inclusion, and to define the practical limiting boundary onto current constructs. Additionally, from there, certain points and observations can be made, and new questions might be asked for problems that cannot be solved yet, for now. Arguing is the best source of thoughts, as the ancients predating us told of, rather true than not.
## The Descartes argument
Descartes left a lot of his works upon the topic that now is conceived to be related to the philosophy of artificial intelligence, ignoring only the historical namesake itself. Most of his works are of great interest on the 'artificial construct' of the time - the idea and society of the 17th century's Age of Enlightenment's *machina*. In the quest to understand his reasoning, for it holds values even now, and holds specific philosophical reasons in such discourse, we will examine **Discourse on Method**, the one specific piece of work directly target this line of thoughts. One of the first target, will be the distinction between man and machine. Though this has been directly argued above, in the example of Descartes and the machina animated to life, it is worthy of mention that this has not been solved, yet. In fact, one can even interpret the Turing test as one kind of empirical boundary for Descartes' criterion - by setting out a boundary on which human-like reaction can take. Moreover, it presumes the justification and the criterion that we should interdict whenever a similar structure is proposed - would it be reactive enough as a man, years long ago predicted?
## The flavour of "strong" and "weak"
Boden's concept of artificial general intelligence resembles John Searl's "strong AI".

“Weak” AI can be defined as the form of AI that aims at a system able to pass not just the Turing Test (again, abbreviated as TT), but the Total Turing Test (Harnad 1991). In TTT, a machine must muster more than linguistic indistinguishability: it must pass for a human in all behaviors - throwing a baseball, eating, teaching a class, etc. According to Searl, "weak AI" is a computer that can behave as if it were thinking wisely.
**"Strong AI"** is then differently defined as a computer that actually thinks like humans. For a quote:
> ##### Searle, 1990s
> According to strong AI ... the appropriately programmed computer really is a mind, in the sense that computers given the right programs can be literally said to understand and have other cognitive states.

The theme of strong AI was frequently discussed in the late 20th century - however, it became clear that in order for a computer to be a strong AI, it must resolve various difficult problems. The most difficult philosophical problem was **the frame problem**.

The **frame problem** is the problem that an AI cannot autonomously distinguish important factors from unimportant ones when it tries to cope with somethin in a certain situation. The problem arises, for example, when we let AI robots operate in the real world. This problem was proposed by John McCarthy and Patrick J. Hayes in 1969. This is considered a philosophical problem that cannot be merely reduced to a technical problem.

For now, the problem is unresolved, per Boden and specialists. Although there is no consensus about the definition of the frame problem, we could say that this is a problem centered around the question of how we can make an AI memorize the 'tacit knowledge' that almost all human adult can have in a given context.

Take an example of the normal cashier. Theoretically, and perhaps realistically speaking, a high schooler can be trained in a rather hasty fashion to do the job. While doing such work, there are many factors we take for granted. For example, during the process of using the computer to input the amount of cash required per transaction making, there might be a few terminal differences between different interface. The cashier knows that, and adapt to it. If she encounters an angry or hurry customer, the cashier can also act accordingly, without the consideration for the performance. "If the customer is in a hurry, then maybe I should use this or that or skip this". Or even "If I push this button, then the trading screen appears". In fact, there are too much knowledge to be involved in such normal and particularly easy job. However, we do not have to input the knowledge that the stock market will affects the customer's money that you are indeed doing transactions, or the fact that if the customer comes in with a bag of money, then in some cases, there will be missing bills, simply because it is not concerned of such.

Considering this, it becomes clear that there is an infinite amount of knowledge the robot must memorize. Who can make such a list of knowledge, and how is it possible to make the robot memorize them? The reason why this happens is that, when a robot encounters a new situation that it has never experienced, it cannot autonomously judge what kind of coping would be important to itself and what kind of coping would not, and therefore it cannot adequately solve the problem it faces. It is interesting that humans seem to be able to solve this kind of problem.

According to Dreyfus, for traditional AI to have the capacity to solve the frame problem, and become a true AI, it must become the Heideggerian AI, which incorporates Vorhandenheit (presence-at-hand) and Zuhandenheit (readiness-to-hand). Examining Rodney Brooks' robot architecture, he said that the robot respond only to fixed features of the environment, not to context or changing significance. But the robot 'continually referring to its sensors rather than to an internal world model'. Then, would the act of choosing, implementing a good enough sensor which have in itself the impendent sensory power in need, and most importantly have the best data coverage be enough to mitigate this? Since even for human, the sensory power is limited, since our limbs, nervous system and cognitive realization systems are finite, but perhaps we have a major sensor of which brought us the power of solving the frame problem locally, as well as responding to 'context or changing significance'? There might be, but then the question is quite straightforward: What (part) of human exhibits such trait? A further concession if made would be even more in line: which construct would give rise to such desired adaptation in the absence of rigorous human benchmark, but for lower-intelligent animals as comparative instead? And even then, would it be reasonable to call computers no better than the smallest reptiles?
## The Chinese room argument
One of the most prominent critique for the philosophy of "strong AI" is the Chinese Room Argument. Accordingly,
> ##### CRA, Searle 1980s
> CRA is based on a thought-experiment in which Searle himself stars. He is inside a room; outside the room are native Chinese speakers who don't know that Searle is inside it. Searle-in-the-box, like Searle-in-real-life, doesn't know any Chinese, but is fluent in English. The Chinese speakers send cards into the room through a slot; on these cards are written questions in Chinese. The box, courtesy of Searle's secret work therein, returns cards to the native Chinese speakers as output. Searle's output is produced by consulting a rulebook: this book is a lookup table that tells him what Chinese to produce based on what is sent in. To Searle, the Chinese is all just a bunch of --- to use Searle's language --- squiggle-squoggles.

We denote $O$ the observers (Chinese speaker), $i$, $o$ being the input, output accordingly, and the rulebook $P$.

The argument of this experiment is simple - the Searle (in the box) is supposed to be everything a computer can be, and because he doesn't understand Chinese, no computer could have such understanding. Searle is mindlessly moving around, and according to the argument, that's all computers do, fundamentally.

Nowadays, CRA, among AI practitioners, is generally rejected. Among these practitioners, there is John Pollock, who writes:
> ##### Pollock, 1995
> Once (my intelligent system) OSCAR is fully functional, the argument from analogy will lead us inexorably to attribute thoughts and feelings to OSCAR with precisely the same credentials with which we attribute them to human beings. Philosophical arguments to the contrary will be passé.

Still, despite such argument, the relevance of CRA is actually more apparent than ever. The brute fact is that deeply semantic NLP is rarely even pursued, hence the proponents of CRA are certainly not the ones feeling some discomfort in the light of the current state of AI.  Searle would rightly point to any of the success stories of AI, and still able to proclaim that what we want - of an artificially intelligent agent has not been made, and understanding has not been reach, and philosophically, we can't refute it. 

In one way or another, CRA infers with the notion of an *input-output* procedure. In such case,  CRA clearly tells us that a simple, mundane notion of an input-output machine which 'does its tasks' would not be sufficient of receiving the clarification and qualification for being intelligent. Which, by all means and purposes, are true. The construct if given in such circumstances, of the thought experiments, represents, if we took out the part where the argument said Searle is supposed to be everything a computer can be, then it's true that such constructs are false in its claim that it can 'understand'. In fact, relatively simple, a given input-output mechanism, from what was observed from the outside, do not exhibit anything, and do not have the ability to even *think*, regardless that is valid of such. If we are to stand by Descartes arguments, then it is even more of the truth - the system in which the thought experiment was conducted provides it with no capabilities of any such.

Then what would be of the Chinese Room Argument that is worth it to dissect? Well, firstly, the claims of, at least in the acute interpretation - the man in the box is supposed to be everything a computer can be is *false*. IF we are to stand by our construction of facilities, then we are innately arguing about such facilities, and not the processes, and the underlying operations itself. Rather, we are complaining that the machine is not capable enough, which is true. But we also, to a given reference point, pointing to the **existential facilities** instead of the arbitrary, yet reasonable operational facilities instead for the comparison. And in fact, if we think of it in the layer construction, it makes more sense - each layer is classified given its arbitrary for now, an interpretation thereof. Such interpretation is contained for such layer, and hence cannot be thoroughly or at least in a glance, interpreted by lower-layer components. This construct offers a one-way restriction on the property of interpretability. In such case, the **System Reply** is partially right - the man inside can not be, by all means and purposes, understand what does it mean by even 'English', or 'Chinese'. And even if such English 'understanding' is embedded in the reasonable interpretable space of Searle in the room, then Chinese would appear to be entirely unknown, *unless* there exists a helper tool to resolve the situation of undefined operation. Indirectly, this prevents a Descartes situation from happens - an undefined situation with particular way to resolve.

But more than that, what constitute the notion of understanding? Taken only from the setting of the thought experiment, we cannot do but deceptively assume that understanding a language is similar to giving it certain ruleset to transfer from this word to others word only. By that, converting from base-2 to base-10 works the same - you know how to do it, yet there are things that constitute the philosophical, higher-level notion of 'understanding' in such that allows you to actually understand the conversion - otherwise, the conversion is a blind matching. In fact, given the setting, would a conversion rulebook exists for such language? Language is, by itself, a very high-level concept. Translating from text to texts requires it not only to provide the definition matching, which could be reasonably identified by such notion like the rulebook mention, but the interpretation of the string is dictated by the logic of the language - the context in which it appears, the logical conformation that it contributes, the grammatical structure that makes sense of what is said and what is transferred, and else. Language itself, is a medium of information exchange, by one of its definition. A conversion does not constitute an understanding. And if the argument going back and forth is that Searle can somehow figure it out the patterns mean something, then it violates the *principle of externality* - the 'lower components' up to a given point in which the *law of recursive immergence* does (not) apply, cannot implement its higher constructions. Then, the Chinese Room Argument can be interpreted, in somewhat meager form, the argument against telling the current conformation as able to understand, while it is not.

The only thing here that is wrong (okay, I am an amateur, so this is totally biased as shit), however, is the fact that Searl(e) claims that it is all the computer can do. However, artificial intelligence, as for now, using this term since chapter 3 which is not yet here, is not a computer in its form. We say, however, for an *artificial intelligent subject with computers as its existential facilities*, CRA is partially right. But partially wrong since the comparison is limited to a form of internal structure in a well-formed system. That is to said - we need to create the (a) construct(s) that exceed(s) such argument. The problem is, how?

In 1961, J. R. Lucas presents the Gödelian argument against the existence of a "strong" AI. His proof is based on Gödel theorem, which is stated as followed:
> ##### Lucas, 1961
> In any consistent system which is strong enough to produce simple arithmetic there are formulae which cannot be proved-in-the-system, but which we can see to be true. Essentially, we consider the formula which says, in effect, "This formula is unprovable-in-the-system". If this formula were provable-in-the-system, we should have a contradiction: for if it were provable-in-the-system, then it would not be unprovable-in-the-system, so that "This formula is unprovable-in-the-system" would be false: equally, if it were provable-in-the-system, then it would not be false, but would be true, since in any consistent system nothing false can be proved-in-the-system, but only truths.

This theorem holds for all formal systems which are consistent, adequate for simple arithmetic, and shows that those formal systems are incomplete, with some fact being true, but unprovable.

Lucas argues that the theorem must apply to cybernetical machines (or now computers) because
> ##### Lucas, 1961, 2
> It is of the essence of being a machine, that it should be a concrete instantiation of a formal system. It follows that given any machine which is consistent and capable of doing simple arithmetic, there is a formula which is incapable of producing as being true.

Further argued, he then comes to such conclusion that no machine can be a complete or adequate model of the mind, since "the mind are essentially different from machines". Lucas's defenders, Roger Penrose, also state in his *Shadow of the Mind* (1994). A human mathematician, if presented with a sound formal system $F$, could argue as followed:
> ##### Penrose, 1992, 3.2
> Though I don't know that I necessarily am $F$, I conclude that if I were, then the system $F$ would have to be sound and, more to the point, $F'$ would have to be sound, where $F'$ is $F$ supplemented by the further assertion "I am $F$" [^10]. I perceive that it follows from the assumption that I am $F$ that the Gödel argument $G(F')$ would have to be true and, furthermore, that it would not be a consequence of $F'$. But I have just perceived that "If I happen to be $F$, then $G(F')$ would have to be true", and perceptions of this nature would be precisely what $F'$ is supposed to achieve. Since I am therefore capable of perceiving something beyond the powers of $F'$, I deduce that I cannot be $F$ after all. Moreover, this applies to any other system, in place of $F$. 

By default, the argument supplemented from Penrose raised the contradiction of proof-ness. The Godelian argument implicitly creates *layers*, and levels, on which one puts those languages they are abided to seem fit of their expressions on the shelf, by the order of *effectiveness*. Such notion then, would make the advancement of machine to human seems perpetually, unsophisticatedly, inoperable and impossible in essence.

Before even taking a stance on such argument, what is the meaning and interpretations, as well as obstensively why it is even important to divulge into such point? The answers might be a bit difficult.

Human is variedly different from machine, for the current time with all the knowledge at present. Truth to take, the action of writing this itself is part of the endeavour to discover one's self, or rather, to understand $F$ with the assertion of 'I am $F$', for now, that we can, and is doing. By the language and construction of contemporary and propositional logic, a machine cannot do that. In general, we cannot even say that it is true of the truth that human actually differs from what is proposed to be perceiving $F'$ being $F$. For human understanding of ourselves alone, we are trying to fit it into the interpretation and the rough 'understanding' of human itself. That is, there exists the space of reason and argument of a scientist, which interpretation follows. If, supposedly, this interpretation is strong enough, then we might be able to perceive and understand ourselves from ourselves - a looped interpretability. This mechanism, if ever, is not well understood if exists. 

However, if the converse situation happens, where we cannot totally perceive what we actually think, and how it is formed - per metric, being either consciousness, or one's self, surprisingly, it does not support the previous argument from an intuitive view (Bear in mind that this is a non-rigorous study). If stays rigid as it is, not counting being dynamic as we want, the model created from a human being can only imitate and represents what directly is entailed in the human mind of interpretation and logics. But logic and interpretation is a construct of the mind, for all intents and purposes, to directly infers to the physical world, the living world. However, if one is to use such inference on itself, for example, examining the brain itself, then to a certain point, what can be deduced from such observation can only fit in the interpretation space of what its creator, the human brain itself, can contrive. Thereby, we might conclude that figuratively, even human cannot understand human itself, from certain perspective. But the quality of succinctively interesting loop is to be taken seriously. The point now is, what type of construct, even logic, would be sufficient of taking the understanding, and will it make uses of the looped behaviors?} By that, we then argue superficially that anything that relies on the machine cannot model the human existence and conscience itself. There are some assumptions thereof in the argument:
- Existences and the state of the world are in fact, modelled in mathematics, for one way or another. This is to facilitate the use of formal system in the argument. Everything is a set of rules, in which things operates.
- A machine per its definition, cybernetical machines are of all expressed by the single principle that it is born out of a formal system itself.
- Truth is the finite quantity that exists in such formal system.
- The mind is an entity of which is inherently different from the logic of formal system.
Those fundamental, overlapping assumptions make up the bulk of the Gödelian argument, from the surface.  

However, is that true of all the merit? [^9]

# The, well, end
Last sections have been quite a mouthful, for what it is worth. We argued, and construct various philosophical perception on the **concept of intelligence** as we want. We also have a lot of time to diverge ourselves to the topic of philosophical and conceptual debate of the construct itself, as for the great mind came before what is written down. However, it is not all what we would actually want. And we are also uncertain of the idiotic choice made along the way.

What we would want, in certain way or another, is an *operational philosophy* instead of the usual mundane philosophical debate one might deem as armchair discussions. By operational, means able to construct and create, try and test, and the principle that might want to follow as it is, even then those can be broken for new principle of a more powerful notion to take place. I have interest in the philosophical debate on higher ground - whether strong AI is sufficient, or the Godelian argument of the treatment of machine as fundamental logical axiomatization. But we simply know nothing. Not even one bit. Discussing with oneself, similar to talking against ourselves, does not guarantee the actual goal that the field or the study of quote, *artificial intelligence* even want - its creation. So before jumping in such term, we might want to do others than philosophy instead. It also seems futile to speak of the end by now when everything is barely started, about logic and the existence of convergence to the human intelligence. By far, we don't even know if logic exists without human, many philosophical questions laid dormant simply because they are born out of thoughts - which in particular, pair no further than between dogs and cats with the realistic necessity. We might want to leave those thoughts.
# References
1. Intelligence: A Brief History. URL: https://www.researchgate.net/publication/242083221_Intelligence_A_Brief_History.
2.  Daniel Estrada, Rethinking machines: artificial intelligence be-yond the philosophy of mind - PhilPapers. URL: https://philpapers.org/rec/ESTRMA.
3.  D.-L. Birgitta. Artificial Consciousness: Misconception(s) of a Self-Fulfilling Prophecy. In B. Dresp, editor, Queios. 2023.
4. Journal of Philosophy of Life Vol.13, No.1 (January 2023): 1-4
5. S. Bringsjord and N. S. Govindarajulu. Artificial Intelligence. In E. N. Zalta and U. Nodelman, editors, The Stanford Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford University, summer 2024 edition, 2024.
6. S. E. of Philosophy. Definitions, n.d. URL: https://plato.stanford.edu/entries/definitions/. 
7. M. Morioka. Artificial Intelligence and Contemporary Philosophy
8.  S. Legg and M. Hutter. A Collection of Definitions of Intelligence, June 2007. URL: http://arxiv.org/abs/0706.3639


___

[^1]: This is not always the case, but nevertheless, we might want to define it. Intention might be following a pattern. Moreover, some can argue that not just a pattern, but a logical deduction of which, for instance of the observable time, there exist causes and consequences, there exist action and consequences, for if one looks back, the string of events usually should not be taken as interaction of potentially randomness, but rather the more probable explanation relies on choice - the choice for those events to happen or not.

[^2]: Clarification for each of those terms, and a general idea of such concept. A definition is made up of two parts: the definiendum and the definiens. The definiendum is the term that is to be defined, whereas the definiens is the group of words or concepts used in the definition that is supposed to have the same meaning as the definiendum.
	
	A *dictionary definition* reports the existing meaning of a term, in one sense of this phrase, and aims to provide definitions that contain sufficient information to impart an 'understanding' of the term, whether this understanding involve operations and overall intuitions of such meaning.
	
	*Stipulative definitions* imparts a meaning to the defined term, but involve no commitment that the assigned meaning agrees with prior uses (if any) of the term; basically, assigning new meanings to a term, whether the term has already got a meaning or not. 
	
	*Descriptive definitions* on the other hand is similar to stipulative, but also aim to be adequate of existing usage.
	
	*Explicative definition* offers neither descriptively nor stipulatively, but as explication, aim to respect some central uses of a term, but stipulative on others. This, nevertheless, can be understood as having both, in a sense. The central aspect, however, relies on the fact that explicative definition views identical function with different definiendum the same, as long as the definiens retains its core functions, and truth, for some case the consequences might differ from such, as long as the essential remains.
	
	Finally, *obtensive definitions* while depend on context and experience, its essential function is to enrich certain language, by the mean of limiting the windows of perception to certain object which entails shared attributes - so for example, let $x$ be a number in $\mathbb{Z}$' while we know that there are many other integers. In fact, this way of defining is considered, for some thinkers to be the source of all primitive concepts (Russell, Whiteley). We would be using these notions of definitions in the future.

[^3]: *Operational* means more to empiricism than not - it's the definition motivated by the constraints and corrective observation, values that lie within the world of the realizing manifestation, rather than in the theoretical world of thoughts and theorems.
[^4]: We are using the **dictionary definition** for the machine here, as an apparatus using mechanical power and having several parts, each with a definite function and together performing a particular task.

[^5]: Notice the usage. It is very much presented that the term intelligence, albeit is defined, for us as above being the relative set of quantifications.
[^6]: Definition lacking. Later on need to define what does this mean.
[^7]: Please, in the future, rewrite this section. It sucks, though heuristically (yeah duh) it works. 
[^8]: This conjecture follows from the observation that any sufficiently strong system, will have the ability to construct a logical system of similar complexity to itself in its own operating space. The foremost example is the action of simulating a computer, inside a computer. By the *computational theory of mind*, if we take on the stance that computer to a given complexity is indeed intelligent being, then the conjecture follows.
[^9]: It turns out, however, the Godelian argument has various proponents and opponents, and there are arguments of it being false. See [Bringsjord, 2001] for such argument, but it can be simplified as this. The Godelian argument makes use of two assumptions: $G(F')$ is true for a Godelian statement, and $F'\not\vdash G(F')$ for $F'\not \vdash G(F')$ for $F'$ being "I am $F$" with added semantic. Then, the statement on $G(F')$ is true is nothing but a *satisfaction* claim, of meta-mathematical assertion which can be reduced to $\mathcal{I}\vDash G(F')$ is true for given interpretation $\mathcal{I}$. Thereby, there exists no contradiction thereof.
[^10]:The phrase "I am $F$" is merely a shorthand for "$F$ encapsulates all the humanly accessible methods of mathematical proof"